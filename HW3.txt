ДОМАШНЕЕ ЗАДАНИЕ 3.

https://docs.google.com/document/d/1pmFdtJrFD_hwO6eSO8GYyPRz-RoAPikYxRAZ2AtccZ4/edit?tab=t.0

В данном домашнем задании вам предстоит разобраться с Nvidia Triton Inference Server, научиться экспортировать модели в ONNX и компилировать в TensorRT.

Для удобства, данное домашнее задание разбито на части.

Часть 1.
Для начала вам предстоит выбрать текстовый трансформер из списка (https://docs.google.com/spreadsheets/d/1Cb834Zyeg2NrO88KShiEpOXpqd0kZJSDzkfPMZl6_4Y/edit?usp=sharing. Был выбран  https://huggingface.co/distilbert/distilbert-base-uncasedб N = 2) которую вы будете экспортировать. Это самый простой пункт, на котором вы сможете отработать алгоритм деплоя моделей в тритон. Сперва вам необходимо подготовить .py скрипт конвертирующий предобученную модель в onnx:
Отнаследуйте класс от torch.Module
В методе __init__ проинициализируйте выбранный трансформер как поле класса, а так же проинициализируйте полносвязанный слой переводящий hidden dim в размерность N [N указана рядом с названием трансформера в списке выше]
Метод forward принимает на вход два аргумента (input_ids: torch.Tensor, attention_mask: torch.Tensor). Внутри метода, сначала извлекаются векторные представления токенов с последнего слоя (last hidden state), а затем к ним применяется полносвязанный слой понижающий размерность до N который был проинициализирован в __init__
Подготовьте sample inputs для конвертации (то есть примените токенайзер к некоторому тексту, вы должны получить два тензора который ждет ваша модель в методе forward)
Сконвертируйте в onnx вызовом функции torch.onnx.export. В качестве дополнительных аргументов используйте opset_version=19, dynamic_axes - перечисление названий осей в виде ключей словаря с дин.осью 0, то есть с динамическим размером батча. НАЗВАНИЯ ОСЕЙ - КАПСОМ, осмысленные, например INPUT_IDS, ATTENTION_MASK, EMBEDDING. Названия INPUTS, OUTPUTS, A, B, C, BOB, ALICE - не подойдут
Внутри того же скрипта вызовите санити-чек над сохраненной моделью через onnxruntime session -- аутпуты onnx должны совпадать с аутпутами torch (допустима численная погрешность).
Экспортируйте токенайзер, это можно сделать командой transformers.AutoTokenizer.from_pretrained(<название вашей выбранной модели>).save_pretrained(<путь до директории куда будет сохранено>).
В конце этого задания вам нужно включить в отчет некоторую информацию о вычислительной сложности выбранной модели. Посчитайте суммарное значение FLOP; значения FLOP от каждого слоя модели; выделите два списка - слои ограниченный арифметикой и слои ограниченные памятью (и пороговое значение batch size при котором слой становится ограниченным арифметикой)
Назовите этот скрипт torch2onnx.py
Обязательно наличие main guard под которым происходит вызов единственной функции main()

После того как вы получите onnx, вам нужно сконвертировать его в tensorrt, для этого:
Подготовьте bash скрипт (скрипт с расширением .sh, первой строчкой - шебанг)
В скрипте пропишите вызов поманды trtexec. В качестве minShapes - размеры с батчом 1, в качестве maxShapes - размеры с батчом 8 (optShapes выберите сами какой захотите). Вам необходимо сконвертировать модель 4 раза с разными флагами арифметической точности: [--fp16, --int8, --best, <без флага, то есть FP32>]. Сохраните эти 4 .plan файлика, они понадобятся для включения в ваш текстовый отчет информации о том, как сильно меняются эмбединги модели в зависимости от этих флагов (сравнение с эталонным ONNX)

Часть 2.
Теперь приступайте к созданию triton model_repository
Вам нужно разобраться с токенизацией текста. Она должна происходить внутри тритона. Для этого вам понадобится следующее:
Рядом с директорией model_repository (на том же уровне) создайте директорию assets
Экспортированный токенайзер из torch2onnx.py положите в assets
Внутри model_repository создайте директорию “tokenizer”
Внутри “tokenizer” файл config.pbtxt и директория “1” с файлом model.py (можете использовать пример с семинара)
Создайте в model_repository 5  различных директорий: model_ONNX, model_TRT_FP16, model_TRT_FP32, model_TRT_INT8, model_TRT_BEST, в каждой из которых внутри “1” должен лежать соответствующий onnx или .plan файл. Не забудьте что батч сайз - вариативен, проставьте соответсвтующее значение (8 в нашем случае) в max_batch_size в config.pbtxt.
Далее, внутри model_repository создайте директорию “ensemble” -- это будет ансамбль, соединяющий токенайзер и эмбеддеры. Используйте пример из репозитория. Ансамбль должен возвращать 5 эмбеддингов, которые вам в дальнейшем предстоит сравнить друг с другом и понять как флаги арифметической точности в tensorrt влияют на качество эмбеддингов. Внутри ensemble не забудьте создать директорию “1”,  в ней должен лежать .gitkeep

Теперь вам предстоит написать скрипт client.py, в котором:
Функция call_triton(input_text) вызывает ансамбль в тритоне и возвращает 5 эмбеддингов
Функция check_quality(input_text)  которая вызывает внутри call_triton(input_text) и возвращает отклонения tensorrt эмбеддингов от onnx эмбеддинга
Функция main() которая в цикле вызывает check_quality для набора текстов, усредняет отклонения и выводит результат через print
Обязательно наличие main guard!

Часть 3.
Вам остается только составить текстовый отчет о проделанной работе:
Начните с замера производительности. Используя triton SDK и команду perf_analyzer (было на семинаре), замерьте througput и latency ДЛЯ КАЖДОЙ из 5 моделей в отдельности, concurrency range - от 1 до 32. Полученные числа внесите в текстовый отчет
Внестите данные об отклонение trt эмбеддингов от onnx эмеддингов
Внесите данные об арифметической сложности модели
